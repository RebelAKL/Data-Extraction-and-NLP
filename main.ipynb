{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rebel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return set(words)\n",
    "\n",
    "positive_words = load_word_list(\"MasterDictionary\\\\positive-words.txt\")\n",
    "negative_words = load_word_list(\"MasterDictionary\\\\negative-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    doc = nlp(text)\n",
    "\n",
    "    word_count = len([token for token in doc if not token.is_space])\n",
    "    sentence_count = len(list(doc.sents))\n",
    "\n",
    "    positive_score = sum(1 for token in doc if token.text.lower() in positive_words)\n",
    "    negative_score = sum(1 for token in doc if token.text.lower() in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)\n",
    "    subjectivity_score = (positive_score + negative_score) / (word_count + 1e-6)\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count != 0 else 0\n",
    "\n",
    "    def syllable_count(word):\n",
    "        return len(re.findall(r'[aeiouy]+', word.lower()))\n",
    "\n",
    "    complex_words = [token.text for token in doc if syllable_count(token.text) > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count != 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_word_length = sum(len(token.text) for token in doc) / word_count if word_count != 0 else 0\n",
    "    syllables_per_word = sum(syllable_count(token.text) for token in doc) / word_count if word_count != 0 else 0\n",
    "    personal_pronouns = len([token for token in doc if token.text.lower() in [\"i\", \"we\", \"my\", \"ours\", \"us\"]]) \n",
    "\n",
    "    return {\n",
    "        \"Positive Score\": positive_score,\n",
    "        \"Negative Score\": negative_score,\n",
    "        \"Polarity Score\": polarity_score,\n",
    "        \"Subjectivity Score\": subjectivity_score,\n",
    "        \"Avg Sentence Length\": avg_sentence_length,\n",
    "        \"Percentage of Complex Words\": percentage_complex_words,\n",
    "        \"Fog Index\": fog_index,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Word Count\": word_count,\n",
    "        \"Syllable Per Word\": syllables_per_word,\n",
    "        \"Personal Pronouns\": personal_pronouns,\n",
    "        \"Avg Word Length\": avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles(input_file):\n",
    "    df = pd.read_excel(input_file)\n",
    "    os.makedirs(\"articles\", exist_ok=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row[\"URL_ID\"]\n",
    "        url = row[\"URL\"]\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find('h1').get_text(strip=True)\n",
    "            article_body = soup.find('article').get_text(strip=True)\n",
    "            \n",
    "            with open(f\"articles/{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(title + \"\\n\" + article_body)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract URL_ID {url_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(output_file):\n",
    "    results = []\n",
    "    for file in os.listdir(\"articles\"):\n",
    "        if file.endswith(\".txt\"):\n",
    "            url_id = file.split(\".\")[0]\n",
    "            with open(f\"articles/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            analysis = analyze_text(text)\n",
    "            analysis[\"URL_ID\"] = url_id\n",
    "            results.append(analysis)\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting articles...\n",
      "Performing text analysis...\n",
      "Text analysis completed. Results saved in Output Data Structure.xlsx.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"Input.xlsx\"\n",
    "    output_file = \"Output Data Structure.xlsx\"\n",
    "\n",
    "    print(\"Extracting articles...\")\n",
    "    extract_articles(input_file)\n",
    "\n",
    "    print(\"Performing text analysis...\")\n",
    "    perform_analysis(output_file)\n",
    "\n",
    "    print(f\"Text analysis completed. Results saved in {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrapping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
